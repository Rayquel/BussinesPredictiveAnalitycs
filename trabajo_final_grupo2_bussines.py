# -*- coding: utf-8 -*-
"""trabajo_final_grupo2_bussines.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1qrmzmNr16rVCkUyCS-2rr0hMqNiAxUCL

## â™¥ğŸ™‚  **Modelo de Machine learning para la prediccion de riesgo de ataque cardiaco** â™¥ğŸ™‚

##**Importamos las librerias** ğŸ“Š

Importamos librerÃ­as para anÃ¡lisis de datos, cÃ¡lculos numÃ©ricos y grÃ¡ficos.
"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

# %matplotlib inline

#Automcompletar rÃ¡pido
# %config IPCompleter.greedy=True
#Desactivar la notaciÃ³n cientÃ­fica
#Desactivar la notaciÃ³n cientÃ­fica
pd.options.display.float_format = '{:.3f}'.format

#Google Drive
from google.colab import drive
drive.mount('/content/drive')

"""##**Google Drive** ğŸ–¥ï¸"""

df = pd.read_csv("/content/drive/MyDrive/Grupo_Bussiness/heart_attack_prediction_dataset.csv", encoding='ISO-8859-1', sep=",")

"""##**Carga de datos** ğŸ”„"""

df.head(4)

df.info()

print(df.shape)

"""#**Deteccion de valores nulos** ğŸ¤”

## Contamos cuÃ¡ntas filas duplicadas hay en el dataset ğŸ¤”
"""

df.duplicated()

df[df.duplicated()]

condicion = df['Patient ID'] == 'BMW7812'
df[condicion]

df.duplicated().sum()

"""Eliminacion de filas duplicadas"""

df.drop_duplicates(inplace=True)
df.shape

"""ğŸ˜„âœ… *Como resultado se puede evidenciar que no hay duplicados* âœ… ğŸ˜„

##Mostramos la cantidad de valores Ãºnicos por columna, ordenados de menor a mayorğŸ”¢
"""

df.nunique().sort_values(ascending=True)

"""âœ… ğŸ˜„ *Resultado: No hay valores Ãºnicos, por lo que todas las columnas aportan valor al anÃ¡lisis* âœ… ğŸ˜„

##Contamos los valores nulos por fila y los ordena de mayor a menor ğŸ”¢
"""

df.isnull().sum(axis=1).sort_values(ascending=False)

df.isnull()

"""##Calculamos el porcentaje de valores nulos por fila âœ–"""

(df.isnull().sum(axis=1)/df.shape[1]*100).sort_values(ascending=False)

"""## Eliminamos filas con un 10% o mas (2.7 o mas) de nulos âŒ"""

df.dropna(thresh=3, inplace=True)
print(df.shape)

"""*Resultado: Como ninguna fila tiene un 10% de nulos, no se eliminÃ³ ninguna* ğŸ˜

#**Aplicando mayusculas a los encabezados** ğŸ” 
"""

columna_renombrar={'age':'Age', ' sex':'Sex','diabetes':'Diabetes'}
df.rename(columns=columna_renombrar, inplace=True)
df.head(4)

"""#**Tratamiento de valores nulos** âš ï¸

##Creamos un nuevo dataframe (dt_num) que contiene solo las columnas numÃ©ricas. ğŸ”¢
"""

df_num=df.select_dtypes(include='number')
df_num.head(3)

"""##Creamos una copia del dataframe (dt_cat) con solo las columnas categÃ³ricas o no numÃ©ricas ğŸ±"""

df_cat=df.select_dtypes(exclude='number').copy()
df_cat.head(3)

"""Primero trabajamos con df_cat

Vemos cuÃ¡ntos nulos hay en cada columna ğŸ¤”
"""

df_cat.isnull().sum().sort_values(ascending=False)

"""Vemos que porcentaje de los datos representan los nulos en cada columna ğŸ“ˆ"""

df_cat.isnull().sum().sort_values(ascending=False) * 100 / len(df_cat)

print('Variable "District"\n')
print(df_cat['District'].value_counts())
print('\ntotal de nulos: ', df_cat['District'].isnull().sum())
print('total de filas: ', df_cat['District'].count())
print('\nModa: ',df_cat['District'].mode()[0])

"""Imputamos la moda en los nulos cÃ¡lculo
ğŸ“Š
"""

valor_DISTRICT_MODA = df_cat['District'].mode()[0]
df_cat['District'] = df_cat['District'].fillna(valor_DISTRICT_MODA)

df_cat.isna().sum().sort_values(ascending=False)

"""**GrÃ¡fico de barras que muestra la frecuencia de los diferentes estados civiles en el conjunto de datos categÃ³ricos** ğŸ§ğŸ‘«â“"""

ax = df_cat['Marital Status'].value_counts().plot(kind='bar');
ax.set_ylabel("Frecuencia")
ax.set_xlabel("GÃ©nero")

"""Ahora trabajamos con dt_num ğŸ”¢"""

print(df_num.shape)
df_num.isna().sum().sort_values(ascending=False)

df.info()

"""MODELO REGRESION LOGISTICA HOMBRES

# EDA

#ğŸ“Š Importamos la librerÃ­a Seaborn**, que utilizaremos para crear grÃ¡ficos estadÃ­sticos mÃ¡s atractivos y fÃ¡ciles de interpretar
"""

import seaborn as sns

"""## Mostramos un grÃ¡fico de barras que representa la cantidad de pacientes con y sin riesgo de infarto. ğŸ©º"""

sns.countplot(data=df, x='Heart Attack Risk')
plt.title('DistribuciÃ³n del Riesgo de Infarto en la Base de Datos')
plt.xlabel('Riesgo de Infarto (0 = No, 1 = SÃ­)')
plt.ylabel('Cantidad de Pacientes')
plt.tight_layout()
plt.show()

porcentaje_riesgo = df['Heart Attack Risk'].value_counts(normalize=True)[1] * 100
print(f"{porcentaje_riesgo:.2f}% de los pacientes tienen riesgo de infarto")

"""*Resultado: Vemos que el 35.28% de nuestros pacientes tienen riesgo de infarto* ğŸ“ˆ

## Mostramos un grÃ¡fico de barras que representa la cantidad de hombres y mujeres dentro del grupo con riesgo de infarto ğŸš»
"""

riesgo = df[df['Heart Attack Risk'] == 1]
sns.countplot(data=riesgo, x='Sex')
plt.title('DistribuciÃ³n por Sexo entre los Pacientes con Riesgo de Infarto')
plt.xlabel('Sexo')
plt.ylabel('Cantidad')
plt.tight_layout()
plt.show()

"""*Resultado: De ese 35.28% vemos que una gran mayorÃ­a es del sexo masculino, siendo estos mÃ¡s del doble que las mujeres* ğŸ‘¨â€âš•ï¸ğŸ‘©â€âš•ï¸

## Creamos un histograma apilado** para visualizar la distribuciÃ³n de edades entre pacientes con riesgo de infarto, separados por sexo. ğŸ“ŠğŸ§‘â€âš•ï¸ğŸ‘©â€âš•ï¸
"""

sns.histplot(data=riesgo, x='Age', hue='Sex', bins=20, kde=True, multiple='stack')
plt.title('DistribuciÃ³n de Edad por Sexo (Riesgo de Infarto)')
plt.xlabel('Edad')
plt.ylabel('Cantidad de Pacientes')
plt.tight_layout()
plt.show()

"""*Resultado: De las mujeres y hombres, vemos que, en nuestro dataset, no hay una edad o rango de edades que sobresalga demasiado. La frecuencia de estas es similar a lo largo de todo el grÃ¡fico, sin llegar a ser estadÃ­sticamente significativo. ğŸ“‰ğŸ‘¥ğŸ“Š*

##GrÃ¡fico de barras que compara la cantidad de fumadores y no fumadores entre pacientes con riesgo de infarto, separados por sexo. ğŸš¬ğŸš­ğŸ“Š
"""

plt.figure(figsize=(8,5))
sns.countplot(data=riesgo, x='Smoking', hue='Sex', palette=['orange','blue'])
plt.title('Fumadores y No Fumadores entre Pacientes con Riesgo, por Sexo')
plt.xlabel('Fuma (0=No, 1=SÃ­)')
plt.ylabel('Cantidad de Pacientes')
plt.legend(title='Sexo')
plt.tight_layout()
plt.show()

"""*Resultado: De las mujeres y hombres, vemos que, en nuestro dataset, no hay una edad o rango de edades que sobresalga demasiado. La frecuencia de estas es similar a lo largo de todo el grÃ¡fico, sin llegar a ser estadÃ­sticamente significativo. ğŸ“ŠğŸ“‰ğŸ‘¥*

##Creamos dos grÃ¡ficos de torta (uno para hombres y otro para mujeres) mostrando la proporciÃ³n de consumo de alcohol ğŸ¥§ğŸ·ğŸ‘¨ğŸ‘©
"""

riesgo_male = riesgo[riesgo['Sex'] == 'Male']
riesgo_female = riesgo[riesgo['Sex'] == 'Female']

counts_male = riesgo_male['Alcohol Consumption'].value_counts().sort_index()
counts_female = riesgo_female['Alcohol Consumption'].value_counts().sort_index()

colors = ['#8fd9b6', '#f9a66e']

fig, axs = plt.subplots(1, 2, figsize=(12,6))

axs[0].pie(counts_male, labels=['No consume', 'Consume'], autopct='%1.1f%%', startangle=90, colors=colors)
axs[0].set_title('Consumo de Alcohol en Hombres con Riesgo')

axs[1].pie(counts_female, labels=['No consume', 'Consume'], autopct='%1.1f%%', startangle=90, colors=colors)
axs[1].set_title('Consumo de Alcohol en Mujeres con Riesgo')

plt.tight_layout()
plt.show()

"""*Resultado: Vemos que tanto en hombres como en mujeres con riesgo de ataque al corazÃ³n, la mayorÃ­a consume alcohol, siendo en hombres ligeramente mayor que en mujeres.* ğŸ·ğŸ“ŠğŸ‘¨ğŸ‘©

##Histograma que muestra la distribuciÃ³n de presiÃ³n arterial entre pacientes con riesgo de infarto, diferenciando por sexo ğŸ©ºğŸ“ŠğŸ‘¨ğŸ‘©
"""

plt.figure(figsize=(8,5))
sns.histplot(data=riesgo, x='Blood Pressure', hue='Sex', kde=True, element='step', stat='density', common_norm=False)
plt.title('DistribuciÃ³n de PresiÃ³n Arterial en Pacientes con Riesgo, por Sexo')
plt.xlabel('PresiÃ³n Arterial')
plt.ylabel('Densidad')
plt.tight_layout()
plt.show()

"""*Resultado: No se observa una gran diferencia en la distribuciÃ³n de presiÃ³n arterial entre hombres y mujeres con riesgo de infarto.* ğŸ©ºğŸ“‰ğŸ‘¨ğŸ‘©

## GrÃ¡fico de barras horizontales apiladas que muestra la distribuciÃ³n de obesidad entre pacientes con riesgo de infarto, diferenciados por sexo. âš–ï¸ğŸ“ŠğŸ‘¨ğŸ‘©
"""

ct = pd.crosstab(riesgo['Obesity'], riesgo['Sex'])
ct.plot(kind='barh', stacked=True, color=['#1f77b4','#ff7f0e'])
plt.title('Pacientes con Riesgo: Obesidad por Sexo')
plt.xlabel('Cantidad')
plt.ylabel('Obesidad (0=No, 1=SÃ­)')
plt.legend(['Hombre', 'Mujer'])
plt.tight_layout()
plt.show()

"""*Resultado: El nÃºmero total de pacientes obesos es similar al de no obesos, pero en ambos casos las mujeres son mayorÃ­a.* âš–ï¸ğŸ‘©ğŸ“Š

## Boxplot que muestra la distribuciÃ³n del nivel de estrÃ©s entre hombres y mujeres con riesgo de infarto. Permite comparar la mediana, dispersiÃ³n y posibles valores atÃ­picos. ğŸ“¦ğŸ“‰ğŸ‘¨ğŸ‘©
"""

riesgo_1 = df[(df['Heart Attack Risk'] == 1)]

plt.figure(figsize=(8,6))
sns.boxplot(data=riesgo_1, x='Sex', y='Stress Level')
plt.title('DistribuciÃ³n del Nivel de EstrÃ©s por Sexo en Pacientes con Riesgo de Infarto')
plt.xlabel('Sexo')
plt.ylabel('Nivel de EstrÃ©s')
plt.tight_layout()
plt.show()

"""*Resultado: Se aprecia que en promedio las mujeres con riesgo de ataque al corazÃ³n presentan mayor nivel de estrÃ©s que los hombre* ğŸ˜ŸğŸ“ˆğŸ‘©ğŸ‘¨

##Boxplot que compara la cantidad de dÃ­as de actividad fÃ­sica por semana entre hombres y mujeres con riesgo de infarto. ğŸƒâ€â™‚ï¸ğŸ“¦ğŸ‘¨ğŸ‘©
"""

plt.figure(figsize=(8,6))
sns.boxplot(data=riesgo_1, x='Sex', y='Physical Activity Days Per Week')
plt.title('DÃ­as de Actividad FÃ­sica por Semana segÃºn Sexo en Pacientes con Riesgo de Infarto')
plt.xlabel('Sexo')
plt.ylabel('DÃ­as de Actividad FÃ­sica por Semana')
plt.tight_layout()
plt.show()

"""*Resultado: Vemos que el promedio de dÃ­as que se ejercitan hombres y mujeres es muy similar, pero las mujeres tienen una variabilidad mÃ¡s alta, unas se ejercitan muy poco y otras mucho, mientras que los hombres tienen una actividad fÃ­sica mÃ¡s consistente, sin tantos casos extremos.* ğŸƒâ€â™€ï¸ğŸ“ŠğŸ‘¨ğŸ‘©

## Creamos una matriz de correlaciÃ³n entre las variables numÃ©ricas del dataset mÃ©dico para identificar posibles relaciones lineales entre ellas. ğŸ“ŠğŸ§ ğŸ“ˆ
"""

numerical_cols = df.select_dtypes(include=['number']).columns
df_corr = df[numerical_cols].corr()

plt.figure(figsize=(12, 10))
sns.heatmap(df_corr, annot=True, cmap='coolwarm', fmt=".2f")
plt.title('Matriz de CorrelaciÃ³n de Variables NumÃ©ricas')
plt.show()

"""#Regresion Logistica ğŸ“ŠğŸ¤–

##Paso 1: Importar librerÃ­as necesarias ğŸ“¦
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import (
    accuracy_score, roc_auc_score, classification_report,
    confusion_matrix, roc_curve
)
from sklearn.impute import SimpleImputer

"""##Paso 2: Filtrar el dataset a hombres ğŸ‘¨â€ğŸ’¼ğŸ§ª"""

print("Valores en columna 'Sex':", df['Sex'].unique())
df_male = df[df['Sex'] == 'Male'].copy()

"""##Paso 3: Corregir nombres de columnas y convertir variables a numÃ©rico ğŸ› ï¸ğŸ”¢"""

df_male.rename(columns={"diabates":"Diabetes"}, inplace=True)
cols_to_numeric = ['BMI', 'Sedentary Hours Per Day', 'Exercise Hours Per Week']
for col in cols_to_numeric:
    df_male[col] = pd.to_numeric(df_male[col], errors="coerce")

"""##Paso 4: Seleccionar variables relevantes ğŸ¯ğŸ“Œ"""

# Lista de columnas que queremos conservar
features = [
    'Age', 'Cholesterol', 'Heart Rate', 'Diabetes', 'Family History',
    'Smoking', 'Alcohol Consumption', 'Exercise Hours Per Week',
    'Diet', 'Previous Heart Problems', 'Medication Use', 'Stress Level',
    'BMI', 'Sedentary Hours Per Day', 'Income', 'Triglycerides',
    'Physical Activity Days Per Week', 'Sleep Hours Per Day',
    'Hemisphere', 'Heart Attack Risk'
]

# Asegurarse de que existan en el DataFrame
features = [col for col in features if col in df_male.columns]

# Crear nuevo DataFrame con las columnas seleccionadas
df_male_model = df_male[features].copy()

"""##Paso 5: Convertir variables categÃ³ricas a dummies ğŸ§©â¡ï¸ğŸ”¢"""

df_male_model = pd.get_dummies(df_male_model, columns=['Diet', 'Hemisphere'], drop_first=True)

"""##Paso 6: Separar X e Y (variables predictoras y variable objetivo) âœ‚ï¸ğŸ“ˆğŸ¯"""

# Separar variables predictoras (X) y variable objetivo (y)
X = df_male_model.drop('Heart Attack Risk', axis=1)
y = df_male_model['Heart Attack Risk']

"""##Paso 7: Preprocesamiento robusto (imputaciÃ³n y limpieza) ğŸ§¼ğŸ› ï¸ğŸ“Š"""

# Eliminar columnas completamente vacÃ­as
X = X.dropna(axis=1, how="all")

# Imputar valores faltantes con la media
imputer = SimpleImputer(strategy="mean")
X = pd.DataFrame(imputer.fit_transform(X), columns=X.columns)

"""##Paso 8: Separar conjunto de entrenamiento y prueba ğŸ§ªğŸ§ ğŸ“‰"""

# Separar datos en train y test
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.25, random_state=42
)

# Escalar solo para regresiÃ³n logÃ­stica
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

"""##Paso 9: Entrenar y evaluar modelo de RegresiÃ³n LogÃ­stica ğŸ¤–ğŸ“Šâœ…"""

# Entrenar modelo con peso balanceado
log_model = LogisticRegression(random_state=42, max_iter=1000, class_weight='balanced')
log_model.fit(X_train_scaled, y_train)

# Predicciones
log_pred = log_model.predict(X_test_scaled)
log_proba = log_model.predict_proba(X_test_scaled)[:, 1]

# MÃ©tricas
log_acc = accuracy_score(y_test, log_pred)
log_auc = roc_auc_score(y_test, log_proba)
precision = precision_score(y_test, log_pred)
recall = recall_score(y_test, log_pred)
f1 = f1_score(y_test, log_pred)

# Mostrar resultados
print("\n===== REGRESIÃ“N LOGÃSTICA =====")
print(f"{'Accuracy:':<12} {log_acc:.3f}")
print(f"{'AUC:':<12} {log_auc:.3f}")
print(f"{'Precision:':<12} {precision:.3f}")
print(f"{'Recall:':<12} {recall:.3f}")
print(f"{'F1-score:':<12} {f1:.3f}")
print("\nReporte de ClasificaciÃ³n:")
print(classification_report(y_test, log_pred))
print("Matriz de ConfusiÃ³n:")
print(confusion_matrix(y_test, log_pred))

"""# RANDOM FOREST ğŸŒ²ğŸ§ âœ¨

##Paso 1: Importar librerÃ­as necesarias ğŸ“¦ğŸ§ª
"""

from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import (
    accuracy_score, classification_report, confusion_matrix,
    roc_auc_score, roc_curve
)
from sklearn.impute import SimpleImputer
import matplotlib.pyplot as plt
import pandas as pd

"""##Paso 2: Filtrar solo hombres y corregir columna ğŸ‘¨â€ğŸ’¼ğŸ§¼"""

# Filtrar solo registros con sexo masculino
df_male = df[df['Sex'] == 'Male'].copy()

# Corregir el nombre mal escrito de la columna 'diabates'
df_male.rename(columns={"diabates":"Diabetes"}, inplace=True)

"""## Paso 3: Seleccionar variables relevantes y codificar ğŸ¯ğŸ”¡â¡ï¸ğŸ”¢"""

# SelecciÃ³n de columnas a usar
features = [
    'Age', 'Sex', 'Cholesterol', 'Heart Rate', 'Diabetes', 'Family History',
    'Smoking', 'Alcohol Consumption', 'Previous Heart Problems',
    'Medication Use', 'Stress Level', 'Triglycerides',
    'Physical Activity Days Per Week', 'Sleep Hours Per Day', 'Hemisphere',
    'Heart Attack Risk'
]

# Subset del DataFrame
df_model = df_male[features].copy()

# Codificar variable 'Sex': Female=0, Male=1
df_model['Sex'] = df_model['Sex'].map({'Female': 0, 'Male': 1})

# Codificar variable 'Hemisphere' usando one-hot encoding
df_model = pd.get_dummies(df_model, columns=['Hemisphere'], drop_first=True)

"""## Paso 4: Preparar variables X (predictoras) e Y (objetivo) ğŸ§©ğŸ¯ğŸ“Š"""

# Separar variables predictoras y objetivo
X = df_model.drop('Heart Attack Risk', axis=1)
y = df_model['Heart Attack Risk']

# Imputar valores nulos con la media
imputer = SimpleImputer(strategy="mean")
X = pd.DataFrame(imputer.fit_transform(X), columns=X.columns)

# Dividir en conjunto de entrenamiento y prueba
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.25, random_state=42, stratify=y
)

"""## Paso 5: Entrenar y evaluar modelo Random Forest ğŸŒ²ğŸ¤–ğŸ“ˆâœ…"""

# Entrenamiento del modelo Random Forest
rf_model = RandomForestClassifier(
    n_estimators=200,
    random_state=42,
    class_weight="balanced"
)
rf_model.fit(X_train, y_train)

# Predicciones
y_pred = rf_model.predict(X_test)
y_proba = rf_model.predict_proba(X_test)[:, 1]

# MÃ©tricas de evaluaciÃ³n
acc = accuracy_score(y_test, y_pred)
auc = roc_auc_score(y_test, y_proba)
precision = precision_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)

print("\n===== RANDOM FOREST SOLO HOMBRES =====")
print(f"Accuracy:  {acc:.3f}")
print(f"AUC:       {auc:.3f}")
print(f"Precision: {precision:.3f}")
print(f"Recall:    {recall:.3f}")
print(f"F1-score:  {f1:.3f}")
print("Reporte de clasificaciÃ³n:")
print(classification_report(y_test, y_pred))
print("Matriz de ConfusiÃ³n:")
print(confusion_matrix(y_test, y_pred))

"""# Modelo KNN (K-Nearest Neighbors) ğŸ“ğŸ‘¥ğŸ”

##Paso 1: Importar librerÃ­as necesarias ğŸ“š
"""

from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.impute import SimpleImputer
from sklearn.metrics import (
    accuracy_score, classification_report, confusion_matrix,
    roc_auc_score, roc_curve
)
import matplotlib.pyplot as plt
import pandas as pd

"""##Paso 2: Filtrar solo hombres ğŸ‘¨"""

# Filtrar registros masculinos
df_male = df[df['Sex'] == 'Male'].copy()

# Renombrar columna mal escrita
df_male.rename(columns={"diabates":"Diabetes"}, inplace=True)

"""##Paso 3: Seleccionar variables relevantes ğŸ¯"""

# Lista de columnas seleccionadas
features = [
    'Age', 'Sex', 'Cholesterol', 'Heart Rate', 'Diabetes', 'Family History',
    'Smoking', 'Alcohol Consumption', 'Previous Heart Problems',
    'Medication Use', 'Stress Level', 'Triglycerides',
    'Physical Activity Days Per Week', 'Sleep Hours Per Day', 'Hemisphere',
    'Heart Attack Risk'
]

# Subset del DataFrame
df_model = df_male[features].copy()

# CodificaciÃ³n de sexo
df_model['Sex'] = df_model['Sex'].map({'Female': 0, 'Male': 1})

# CodificaciÃ³n de Hemisferio
df_model = pd.get_dummies(df_model, columns=['Hemisphere'], drop_first=True)

"""## Paso 4: Preparar X e y ğŸ§ª"""

# Separar variables predictoras y variable objetivo
X = df_model.drop('Heart Attack Risk', axis=1)
y = df_model['Heart Attack Risk']

# Imputar valores faltantes con media
imputer = SimpleImputer(strategy="mean")
X = pd.DataFrame(imputer.fit_transform(X), columns=X.columns)

# Escalar variables
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Separar en entrenamiento y prueba
X_train, X_test, y_train, y_test = train_test_split(
    X_scaled, y, test_size=0.25, random_state=42, stratify=y
)

"""## Paso 5: Modelo KNN ğŸ§ """

knn_model = KNeighborsClassifier(n_neighbors=5)
knn_model.fit(X_train, y_train)

# Predicciones
y_pred = knn_model.predict(X_test)
y_proba = knn_model.predict_proba(X_test)[:, 1]

# MÃ©tricas individuales
acc = accuracy_score(y_test, y_pred)
auc = roc_auc_score(y_test, y_proba)
precision = precision_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)

# Mostrar resultados
print("\n===== KNN SOLO HOMBRES =====")
print(f"Accuracy:  {acc:.3f}")
print(f"AUC:       {auc:.3f}")
print(f"Precision: {precision:.3f}")
print(f"Recall:    {recall:.3f}")
print(f"F1-score:  {f1:.3f}")
print("\nReporte de clasificaciÃ³n:")
print(classification_report(y_test, y_pred))
print("Matriz de ConfusiÃ³n:")
print(confusion_matrix(y_test, y_pred))

"""# Optimizacion

##Feature Selection
"""

import pandas as pd
from sklearn.ensemble import RandomForestClassifier
from sklearn.feature_selection import SelectFromModel

X = df.drop(['Heart Attack Risk'], axis = 1)
y = df['Heart Attack Risk']

from sklearn.model_selection import train_test_split

X_train,X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)

non_numeric_columns = X_train.select_dtypes(include=['object']).columns
print("Columnas con valores no numÃ©ricos:", non_numeric_columns.tolist())

print(X_train[non_numeric_columns].head())

X_train = X_train.drop(columns=non_numeric_columns)
X_test = X_test.drop(columns=non_numeric_columns)

#se utiliza para realizar la selecciÃ³n de caracterÃ­sticas (feature selection) en conjuntos de datos. SelectFromModel se utiliza como un clasificador de bosque aleatorio
sel = SelectFromModel(RandomForestClassifier(n_estimators = 1000)) #, threshold=0.03)
sel.fit(X_train, y_train)

sel.get_support()

selected_feat= X_train.columns[(sel.get_support())]
selected_feat

print(len(selected_feat))

sel.estimator_.feature_importances_

sel.estimator_.feature_importances_.mean()

fselection = pd.DataFrame({
    'feature': X_train.columns,
    'importance': sel.estimator_.feature_importances_
})

# Ordenamos por importancia
fselection.sort_values(by='importance', ascending=False, inplace=True)
fselection

"""# Hiperparametrizacion"""

X = df.drop(['Heart Attack Risk'], axis = 1)
y = df['Heart Attack Risk']

from sklearn.model_selection import train_test_split

X_train,X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)

from sklearn.ensemble import RandomForestClassifier

algoritmo_rf = RandomForestClassifier()

grid_parameters =   { 'criterion'        : ['giny','entropy'], # Gini y entropy sirven medir la impureza de un nodo aleatoriamente
                      'max_depth'        : [10, 14, 20], #profundidad del arbol
                      'max_features'     : [10,20], #cantidad mÃ¡xima de caracterÃ­sticas que se consideran para dividir un nodo en un Ã¡rbol de decisiÃ³n
                      'min_samples_leaf' : [2, 4],  #nÃºmero mÃ­nimo de muestras requeridas  para formar una hoja
                      'min_samples_split': [5, 10], #nÃºmero mÃ­nimo de muestras requeridas para realizar una divisiÃ³n en un nodo interno
                      'n_estimators'     : [100, 500], #Crear un clasificador de bosque aleatorio con N# estimadores
                      'n_jobs'           : [-1]} #(n_jobs) -1 para usar todos los nÃºcleos disponibles

random_parameters = { 'criterion'        : ['giny','entropy'], # Gini y entropy sirven medir la impureza de un nodo aleatoriamente
                      'max_depth'        : [5, 10, 14, 20, 30],  #profundidad del arbol
                      'max_features'     : [10,2],  #cantidad mÃ¡xima de caracterÃ­sticas que se consideran para dividir un nodo en un Ã¡rbol de decisiÃ³n
                      'min_samples_leaf' : [2, 4],  #nÃºmero mÃ­nimo de muestras requeridas  para formar una hoja
                      'min_samples_split': [5, 10],  #nÃºmero mÃ­nimo de muestras requeridas para realizar una divisiÃ³n en un nodo interno
                      'n_estimators'     : [100, 500, 1000]} #Crear un clasificador de bosque aleatorio con N# estimadores

from sklearn.model_selection import GridSearchCV
from sklearn.model_selection import RandomizedSearchCV

grid_search_rf = GridSearchCV(
                                      estimator           = algoritmo_rf, #objeto con el algoritmo que se va a entrenar nuestro modelo
                                      param_grid          = grid_parameters,
                                      scoring             = 'roc_auc',#mÃ©trica que serÃ¡ evaluado para tener una mejor precisiÃ³n
                                      n_jobs              = -1,
                                      cv                  = 3,
                                      verbose             = 0
                                     )

random_search_rf = RandomizedSearchCV(
                                      estimator           = algoritmo_rf,
                                      param_distributions = random_parameters,
                                      n_iter              = 10,
                                      scoring             = 'roc_auc', #mÃ©trica que serÃ¡ evaluado para tener una mejor precisiÃ³n
                                      n_jobs              = -1,
                                      cv                  = 3,
                                      verbose             = 0
                                     )

"""#GridSearch"""

non_numeric = X_train.select_dtypes(include=['object']).columns
print("Columnas no numÃ©ricas:", non_numeric.tolist())

X_train = X_train.drop(columns=non_numeric)
X_test = X_test.drop(columns=non_numeric)

X_train = pd.get_dummies(X_train, drop_first=True)
X_test = pd.get_dummies(X_test, drop_first=True)

# Asegurar columnas iguales en train/test
X_test = X_test.reindex(columns=X_train.columns, fill_value=0)

from sklearn.model_selection import GridSearchCV
from sklearn.ensemble import RandomForestClassifier

param_grid = {
    'n_estimators': [100, 200],
    'criterion': ['gini', 'entropy'],  # corregido
    'max_depth': [None, 10, 20],
}

grid_search_rf = GridSearchCV(
    estimator=RandomForestClassifier(random_state=42),
    param_grid=param_grid,
    cv=5,
    scoring='accuracy',
    n_jobs=-1
)

# Entrenamiento con datos corregidos
mejor_modelo_rf = grid_search_rf.fit(X_train, y_train)

print('ConfiguraciÃ³n de los mejores parÃ¡metros:')
mejor_modelo_rf.best_params_

print(f'Resultado de la mÃ©trica {mejor_modelo_rf.scoring} de la mejor configuraciÃ³n de parÃ¡metros:')

mejor_modelo_rf.best_score_

from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, roc_auc_score
import pandas as pd

# Predicciones del mejor modelo
y_pred_rf = mejor_modelo_rf.predict(X_test)
y_proba_rf = mejor_modelo_rf.predict_proba(X_test)[:, 1]

# CÃ¡lculo de mÃ©tricas
acc_rf = accuracy_score(y_test, y_pred_rf)
f1_rf = f1_score(y_test, y_pred_rf)
prec_rf = precision_score(y_test, y_pred_rf)
rec_rf = recall_score(y_test, y_pred_rf)
auc_rf = roc_auc_score(y_test, y_proba_rf)

# Guardamos en DataFrame
results_rf_grid = pd.DataFrame(
    [['Random Forest GridSearch', acc_rf, f1_rf, prec_rf, rec_rf, auc_rf]],
    columns=['Model', 'Accuracy', 'F1', 'Precision', 'Recall', 'AUC']
)

# Mostramos resultados
print(results_rf_grid)

"""# RandomSearch"""

#ExploraciÃ³n Aleatoria: RandomizedSearchCV realiza una bÃºsqueda aleatoria en el espacio de hiperparÃ¡metros. Se seleccionan un nÃºmero fijo de combinaciones de hiperparÃ¡metros al azar y se evalÃºan mediante validaciÃ³n cruzada.
#Espacio Continuo o Discreto: Es mÃ¡s flexible y puede manejar tanto espacios continuos como discretos de hiperparÃ¡metros.
#Eficiente para Espacios Amplios: Puede ser mÃ¡s eficiente que GridSearchCV cuando el espacio de bÃºsqueda es grande, ya que no evalÃºa todas las combinaciones posibles.

mejor_modelo_rf = random_search_rf.fit(X_train,y_train)

#print('\nConfiguraciÃ³n de todos los valores del grid:')
#mejor_modelo_rf

print('ConfiguraciÃ³n de los mejores parÃ¡metros:')
mejor_modelo_rf.best_params_

print(f'Resultado de la mÃ©trica {mejor_modelo_rf.scoring} de la mejor configuraciÃ³n de parÃ¡metros:')

mejor_modelo_rf.best_score_

#Modelo Final: Entrenar con los mejores parÃ¡metros a todo TRAIN
algoritmo_rf = RandomForestClassifier(criterion='entropy', max_depth=14, max_features=2,
                       min_samples_leaf=4, min_samples_split=5,
                       n_estimators=100, n_jobs=-1)
algoritmo_rf.fit(X_train, y_train)

y_pred_rf_hy       = algoritmo_rf.predict(X_test)
y_pred_rf_proba_hy = algoritmo_rf.predict_proba(X_test)
y_pred_rf_proba_hy[:,1][:10]

from sklearn.metrics import roc_auc_score,confusion_matrix,f1_score,classification_report,\
                            accuracy_score,precision_score,recall_score

acc_rf_hy  = accuracy_score(y_test,y_pred_rf_hy)
f1_rf_hy   = f1_score(y_test,y_pred_rf_hy)
prec_rf_hy = precision_score(y_test, y_pred_rf_hy)
rec_rf_hy  = recall_score(y_test, y_pred_rf_hy)
auc_rf_hy  = roc_auc_score(y_test,y_pred_rf_proba_hy[:,1])

resultsrf_hy = pd.DataFrame([['Random Forest Hyperparameters', acc_rf_hy,f1_rf_hy,prec_rf_hy,rec_rf_hy,auc_rf_hy]],
                       columns = ['Model','Accuracy','F1','Precision','Recall','AUC'])
resultsrf_hy

cm_rf_hy = confusion_matrix(y_test,y_pred_rf_hy)
print(cm_rf_hy)